---
layout: post
title: 《Designing Data-Intensive Application》读书笔记
categories: Distributed system
description: 《Designing Data-Intensive Application》读书笔记
keywords: Data-Intensive
excerpt: Data-Intensive Application
---

# 数据系统的基石
## 可靠性，可扩展性，可维护性
### 可靠性
fault和failure不同，fault意思是系统的一部分状态偏离其标准，而failure则是系统作为一个整体停止向用户提供服务。fault的概率不可能降到零，因此需要一定的容错机制来防止fault导致failure

有时在一些容错系统中需要通过故意触发来提高故障率，来确保容错机制不断运行并接受考验，从而提高故障自然发生时系统能正确处理的信心

分布式系统的故障大致可分为：硬件故障、软件错误和人为错误（如配置错误）

### 可扩展性
这一部分需要关注负载参数和性能指标

负载参数可以是是每秒向Web服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的用户数量、缓存命中率等

对于在线系统，通常考虑的性能指标是response time，即客户端发送请求到接收响应之间的时间。需要注意的是response time不同于latency（包含了latency）

Tail latencies（high latencies that clients see fairly infrequently）非常重要，这是因为请求响应最慢的客户往往也是数据最多的客户

排队延迟（queueing delay）占了Tail latencies的很大一部分，只要有少量缓慢的请求就能阻碍后续请求的处理

应对负载需要将vertical scaling（换更好的机器）和horizontal scaling（将负载分布到多台小机器上）灵活地结合

### 无状态服务 (Stateless Service)
无状态服务指的是每个请求都是独立的，服务器不保留任何关于客户端的会话信息。每个请求都包含了服务器处理所需的所有信息。

例如：RESTful API

# 数据模型与查询语言
## NoSQL
NoSQL数据库是一类非关系型数据库，专门设计用于处理大量的数据和高并发的应用程序需求

特点：不使用固定的表格结构，而是采用灵活的数据模型，如键-值对、文档、列族、图等。

### NoSQL数据库的主要类型
1. 键-值存储（Key-Value Store）

    特点：通过键-值对来存储数据，适用于简单的数据存取。
    示例：Redis, Riak

2. 文档存储（Document Store）

    特点：数据以文档的形式存储，通常使用JSON、BSON等格式，每个文档可以有不同的结构。
    示例：MongoDB, CouchDB

3. 列族存储（Column Family Store）

    特点：数据以列簇的形式存储，适用于存储大量的半结构化数据。
    示例：Apache Cassandra, HBase

4. 图数据库（Graph Database）

    特点：使用节点、边和属性来存储数据，特别适合处理复杂的关系和连接。
    示例：Neo4j, ArangoDB

### 优缺点
优点：

* 扩展性强：NoSQL数据库通常支持水平扩展，即通过增加更多的服务器节点来处理更多的数据和请求。

* 高性能：针对特定的查询和存储模式进行了优化，可以提供高并发的读写性能。

* 灵活的数据模型：支持灵活的、动态的模式，无需预定义表结构，适用于快速变化的应用需求。

* 大数据处理：适合处理海量数据，能够存储和处理比关系型数据库更多的数据。

* 高可用性和容错性：许多NoSQL数据库内置了自动分片、复制和容错机制，能够在硬件故障时提供高可用性。

缺点：可能导致数据不一致、查询复杂、事务处理能力有限

# 存储与检索
## 索引
索引是一种数据结构，用于快速查询数据库表中的数据。基本思想是通过保存一些额外的元数据作为路标来快速找到想要的数据

### 常见索引类型
1. 哈希索引：通过哈希函数将键值映射到哈希表中的具体位置，比较适用于等值查询。优点是结构相对简单，通常占用更少的存储空间

2. B树索引

3. bitmap索引

4. 复合索引

## Bitcask

### 工作原理
Bitcask 将所有写操作（如插入、更新）记录到日志文件（write-ahead log）的末尾。这些日志文件按顺序写入磁盘，每个日志文件被称为一个data file。一个Bitcask实例是一个目录，一个目录里可能有多个data files，但只能有一个active data file（用于服务器写入数据），且同一时刻只能有一个进程操作该目录。当active file大小达到一个阈值时会被关闭，同时创建新的active data file。旧的data file是不可变和不可写的。

**Bitcask实例 = 一个active data file+多个旧的data file**

### 为何写入数据很快
Bitcask采用append-only的写入方式，这是因为机械硬盘（HDD）的顺序写入速度远快于随机写入，因为顺序写入只需磁盘旋转和磁头移动一次，而随机写入需要多次寻址，所以这么做能够避免多余的磁盘寻址。Bitcask只有一个writer thread，简化了并发控制

### 为何读取数据很快
Bitcask维护一个**保存于内存中**的哈希索引，这个哈希索引将键映射到对应的数据文件及其在文件中的偏移量。通过这个哈希索引，可以快速定位到存储在磁盘上的数据

Bitcask会定期在后台执行merge操作以清理无用数据，merge过程就是遍历所有的旧的data files，然后将所有有效（没有被删除）的键的最新版本写入到新的文件中，最后再将旧的data files删除。此外，还会进行压缩（compaction）操作，即在日志中丢弃重复的键，只保留每个键的最近更新。

Bitcask的删除是逻辑删除，即追加一次删除记录，在下一次merge时才真正物理删除

### 数据备份和恢复
Bitcask*定期将Keydir的快照保存到磁盘*，以便在系统崩溃后快速恢复，同时也会定期将数据文件复制到安全的位置，以确保数据的持久性

### 适用场景
适用于*高写入吞吐量*，需要*快速读取*，数据更新频率相对较低

### 局限性
由于keydir得存在内存中，因此适用的*数据规模有限*，*键不能太多*。*不支持范围查询*。单线程写入*不适合高并发场景*。由于每次更新都会生成新的数据版本，旧版本的数据需要等待合并和清理操作才能被删除，可能导致*磁盘空间利用率*不高。

## LSM树（Log Structured Merge Trees）
*注意，LSM树不是一种树状数据结构，而是一种存储结构*

两个重要组成部分：

1. SSTable（Sorted String Table）：有序键值对集合。集合中的键是按顺序排列的。SSTable是LSM树组在磁盘中的数据结构。SSTable有两个优势：

* 合并段简单而高效：比较像归并排序，在合并时并排读取输入文件，查看每一个键并将最低的键输出到输出文件
* 无需保存内存中所有键的索引，每几千字节的段文件就有一个键就足够了，其他键可以从已知键的偏移位置处开始扫描

2. Memtable（内存表）。Memtable用于将数据按键排序成为SSTable，它是*位于内存中的某种平衡树数据结构（例如，红黑树或Skip List）*。当内存表大于某个阈值（通常为几兆字节）时，将其作为SSTable文件写入磁盘。新的SSTable文件成为数据库的最新部分。读取数据时，先在内存表中找，若未找到则依次查找最近的磁盘段，直到找到该关键字为止。

磁盘中的SSTable其实就是LSM树的1-n层子树，它们的本质是数据排好序后顺序写到磁盘上的文件。每一层的子树都有一个阈值大小，达到阈值后会进行合并，合并过程中按键排序并删除重复的或过时的数据，合并结果写入下一层。

### 读取性能
读取过程：先在memtable中找，找到直接返回。时间复杂度：O(logN), N为memtable中的键值对数量；若未找到，则递归地去磁盘中的多级子树中寻找，时间复杂度：O(L*logM), L为子树数量，M为子树中的键值对数量

### 写入性能
写入Memtable：时间复杂度：O(logN), N为memtable中的键值对数量

刷写内存表到磁盘：O(N)，但这是批量操作，均摊到每次写入操作的时间复杂度为O(1)

### 合并
1. 内存数据写入磁盘（level 0 - level 1）：对内存中的Level 0树进行中序遍历，将数据顺序写入磁盘的Level 1层

2. 磁盘中多个SSTable的合并：当某一层达到阈值时进行块的合并，合并过程会按照键值进行排序，删除重复数据，并保留最新版本的数据，合并后的数据写入新的更高层次的SSTable文件中

### 如何解决内存表尚未写入磁盘数据库就崩溃的情况
可以在磁盘上保存一个单独的日志（称为WAL，即Write-Ahead Log），每个写入都会立即被附加到磁盘上。每当内存表写出到SSTable时，相应的日志都可以被丢弃。

### LSM树的优缺点
优点：增、删、改操作飞快，写吞吐量极大

确定：读取能力较弱，没有过滤器的话要从Level 0一路查询Level n；不擅长区间范围的读操作； 归并操作较耗费资源（大量的磁盘IO）；压缩过程有时会干扰正在进行的读写操作。

### 适用场景
适合*写多读少*、*顺序读取和范围查询*的场景，也适用于分布式系统等可扩展性系统（因为LSM树能够通过增加节点来扩展存储和计算能力）

### 补充内容
压缩（Compaction）：LSM树会定期进行压缩操作，将多个SSTable文件合并成一个新的SSTable文件，以减少磁盘空间占用并提升查询性能。

布隆过滤器（Bloom Filter）：为了加速查询，LSM树常使用布隆过滤器来快速判断某个键是否存在于SSTable中，减少不必要的磁盘读取。

## B树

B树是一棵平衡的M路平衡搜索树，B树和LSM树的相同点在于均保持按键排序的键值对，这有利于高效的键值查找和范围查询。除此之外B树的设计理念非常不同

LSM树将数据库分解为可变大小的段，通常是几兆字节或更大的大小，并且总是按顺序编写段。B树将数据库分解成*固定大小的块*或页面，传统上大小为4KB（有时会更大），并且*一次只能读取或写入一个页面*。这种设计更接近于底层硬件，因为磁盘也被安排在固定大小的块中。

B树的基本底层写操作是用新数据覆盖磁盘上的页面，而LSM树只是附加数据到文件，从不修改文件

### 节点结构
每个节点包含*多个键和子节点指针*（在磁盘里而不是内存）

### 查找
从根节点开始，逐层查找，比较键值并决定沿哪个子节点指针继续查找，直到找到目标键或到达叶节点。时间复杂度为O(log n)

### 插入
*只会在叶节点插入新的键*，如果叶节点已满，需要进行分裂（split）操作，将节点一分为二，并将中间键上移到父节点。如果父节点也满，则继续向上分裂，可能引起树的高度增加。时间复杂度为O(log n)。

### 删除
删除键后，如果某个节点的键数目少于阶数的一半，可能需要合并（merge）或借用（borrow）邻近的兄弟节点的键以保持平衡。时间复杂度为O(log n)。

### 写放大效应（write amplification）
指的是为了写入一小部分数据，实际写入到磁盘的数据量比预期要大。

#### LSM树的写放大
例如，如果有三个SSTable文件每个包含100个键值对，合并操作需要读取这300个键值对，进行合并排序，然后将结果写入一个新的文件。虽然最终结果可能还是100个键值对，但由于合并过程中需要多次读写相同的数据，这就导致了写放大效应

*LSM树通常能够比B树支持更高的写入吞吐量，部分原因是它们有时具有较低的写放大*
## 两种访问模式

### 在线事务处理（OLTP, OnLine Transaction Processing）
OLTP 系统旨在管理和促进大量简短的在线事务处理任务。 它们处理日常的事务操作。访问数据的方式是交互式的

### 在线分析处理（OLAP, OnLine Analytice Processing）
OLAP 系统旨在分析和查询大量历史数据。 它们支持复杂的查询，用于商业智能和数据分析。

|  | OLTP | OLAP |
| :-----:| :----: | :----: |
| 主要用途 | 事务处理 | 数据分析和汇报 |
| 数据操作 |增删改  | 查询 |
| 查询复杂度 |简单  | 复杂 |
| 数据操作 |增删改  | 查询 |
| 主要写入模式 |随机访问，写入要求低延时  | 批量导入（ETL），事件流 |
| 处理的数据 |数据的最新状态（当前时间点）  | 随时间推移的历史事件 |
| 主要衡量指标 |事务吞吐量  | 查询响应速度 |


# 分布式数据
分布式系统采用的是无共享架构（shared-nothing architecture），运行数据库软件的每台机器/虚拟机都称为节点（node）。每个节点只使用各自的处理器，内存和磁盘。节点之间的任何协调，都是在*软件层面*使用传统网络实现的。

数据在多节点上的分布有两种常见的方式：复制与分区

## 复制
复制指的是在几个不同的节点上保存数据的相同副本，可能放在不同的位置。复制的主要问题是如何处理复制数据的变更

### 主从复制
工作原理：
1. 存储数据库副本的节点之一被指定为 领导者（leader），也称为 主库（master|primary） 。当客户端要向数据库写入时，它必须将请求发送给领导者，领导者会将新数据写入其本地存储
2. 每当领导者将新数据写入本地存储时，它也会将数据变更发送给所有的追随者。每个跟随者从领导者拉取日志，并相应更新其本地数据库副本
3. 只有领导者能接受写操作，追随者是只读的

#### 同步复制与异步复制
同步复制：在向用户报告写入成功，并使结果对其他用户可见之前，主库需要等待从库的确认；

异步复制：主库发送消息，但不等待从库的响应

通常，主从复制都是完全异步的。

#### 主库复制的底层原理
1. 基于语句的复制：主库记录下它执行的每个写入请求（语句）并将该语句日志发送给其从库。这种方式不太好，因为有时语句会调用非确定性函数（nondeterministic）或自增列（auto increment）

2. 基于WAL日志的复制：主库通过网络将日志发送给其从库，当从库应用这个日志时，它会建立和主库一模一样数据结构的副本。缺点：日志记录的数据非常底层：WAL包含哪些磁盘块中的哪些字节发生了更改。这使复制与存储引擎紧密耦合。通常不可能在主库和从库上运行不同版本的数据库软件

### 多主复制
单主复制的问题在于如果从库因为各种原因（比如网络中断）无法连接主库，则无法向数据库中写入数据。

解决方案就是允许多个节点接受写入，复制的方式保持不变，接受写入的每个节点都必须将该数据更改转发给所有其他节点。因此，每个主库也同时是别的主库的从库。

#### 多主复制的应用场景
*跨多个数据中心*的多主复制：当数据副本分散在几个不同的数据中心时，可以在每个数据中心都有主库。这样就可以在每个数据中心内部使用普通的主从复制；在数据中心之间，每个数据中心的主库都会将其更改复制到其他数据中心的主库中。

#### 写入冲突的处理策略
多主复制的最大问题就是可能发生写冲突，这个问题难以解决，最好的办法是避免冲突

1. 最后写入胜出（Last Write Wins, LWW）：给每个写入一个唯一的ID（例如，一个时间戳，一个长的随机数，一个UUID或者一个键和值的哈希），挑选最高ID的写入作为胜利者，并丢弃其他写入。缺点：因为只保留了最后一次写入的值，所以可能会丢失一些数据更新

2. 以某种方式将这些冲突值合并在一起

3. 应用程序级别冲突处理：将冲突检测和解决逻辑委托给应用层，由应用根据业务逻辑来决定如何合并冲突的数据


## 分区/分片
如果数据量很大或吞吐量很高，仅仅进行复制是不够的，还需要对数据进行分区/分片（partition）

每条数据属于一个分区，一个分区其实就是一个自己的小型数据库。分区的主要目的是**可扩展性**，因为不同的分区可以放在不共享集群中的不同节点上，查询负载可以分布在多个处理器上。但如果查询比较复杂，需要跨越多个节点进行处理，也会带来问题

分区通常与复制结合使用，即每个分区的副本存储在多个节点上


# 分布式一致性算法
分布式一致性算法是确保在分布式系统中，多个节点之间能够**达成一致的决策或状态**的一类算法

常见算法：Paxos，Raft，Zab，2PC，3PC等

## Raft共识算法

Raft将共识问题分解为三个相对独立的子问题：leader election, log replication和safety

### Raft基本概念
Raft集群包含多个服务器。在任何给定的时间，每个服务器都处于三种状态之一：领导者（leader）、追随者（follower）或候选人（candidate）

* leader：处理客户端请求，并负责将log entry复制到所有follower，以保证一致性；

* follower：不主动发送任何请求，只是响应来自leader和candidate的请求；

* candidate：用于选举新的leader，提出RequestVote请求

Raft将时间分为不同任期（term），用连续的整数编号。每一届任期都以leader election开始，一名或多名candidate竞选leader。如果一个candidate赢得了选举，那么他将在余下的任期内担任leader。在某些情况下，投票分裂会导致选举失败，那么这一届任期就没有leader，新的任期会随机开始，Raft保证一个任期至多有一个leader。

### Leader election
Leader会周期性地向 Followers 发送 heartbeat（empty AppendEntries RPCs）来维持自身的地位。若某个follower超过了设定的election timeout都没有收到heartbeat，则认为leader崩溃了，会开始新一轮选举

初始时，所有服务器以follower身份启动，所以timeout后会开始第一轮选举。

server按照以下步骤进行选举：
1. 自增当前的term value，并切换为candidate
2. vote自己
3. 不断发出RequestVote RPCs来请求其他server vote自己，直到自己赢得选举/其他candidate赢得选举/超时（无人成为leader）

若无人成为leader，则会再次进行选举。

*Raft采用随机election timeout的方式来确保平局的状况很少发生*

#### 节点如何响应RequestVote RPC
投票给candidate：该节点尚未vote，且candidate的日志比自己的日志更新；

拒绝投票：该节点已经vote过了/candidate的term value小于该节点当前的term value

### Log replication
log replication的目标是使raft集群中的所有节点（leader和followers）在其日志中拥有相同的日志条目顺序和内容，从而保证整个系统的一致性。

#### 基本流程
1. 客户端向leader发出写请求（添加、修改数据等）
2. leader将请求作为一个新的log entry添加到自己的log中。

*log entry通常包括操作（如PUT/DELETE），操作的目标，操作的值，log entry的索引和term value*

3. leader通过AppendEntries RPC来复制该log entry到所有follower节点，保证日志的顺序性和一致性

4. follower收到RPC后检查前一个entry的索引和term value和自己的log里面对应的entry是否匹配。若匹配则将新的entry追加到log中并返回成功消息给leader，否咋拒绝该entry，而leader会发送更前面一个log entry，直到匹配上

5. 当leader确认一个log entry已经被超过半数的follower成功复制后，标记该log entry的状态为committed，随后应用该entry到其状态机中，并返回结果给客户端；follower收到leader的commit后也会做同样的事情


### Safety
1. election restrictions：被选举出来的新leader必须包含之前各 Term 内所有已被 Commit 的日志

2. leader宕机：若一个leader在提交一个log entry前宕机了，新leader会继续尝试完成对这一entry的复制和提交

3. follower和candidate宕机：raft会通过无限期的重新尝试来处理这些失败

4. Timing和availability：只要系统满足以下时间要求，raft就能够选出并保持一个稳定的leader

    *broadcastTime << electionTime << MTBF*

### Log compaction
Raft通过保存snapshot来压缩日志，节省空间（删除snapshot前的所有日志）


